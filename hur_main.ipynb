{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import argparse\n",
    "\n",
    "from scripts.nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from notebooks/run_dir.ipynb\n",
      "importing Jupyter notebook from notebooks/helper_fxns.ipynb\n",
      "importing Jupyter notebook from notebooks/data_loader.ipynb\n",
      "importing Jupyter notebook from notebooks/train_val.ipynb\n",
      "importing Jupyter notebook from notebooks/build_network.ipynb\n",
      "importing Jupyter notebook from notebooks/print_n_plot.ipynb\n"
     ]
    }
   ],
   "source": [
    "'''before we import theano anywhere else we want to make sure we specify \n",
    "a unique directory for compiling, so we dont get into a locking issue\n",
    "if we run multiple hur_mains at once on a global file system. Haven't truly implementedthis yet '''\n",
    "from notebooks.run_dir import create_run_dir\n",
    "from notebooks.helper_fxns import dump_hyperparams\n",
    "from notebooks.data_loader import load_data, load_precomputed_data\n",
    "from notebooks.train_val import train\n",
    "from notebooks.print_n_plot import plot_ims_with_boxes\n",
    "from notebooks.build_network import build_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if inside a notebook, then get rid of weird notebook arguments, so that arg parsing still works\n",
    "if any([\"jupyter\" in arg for arg in sys.argv]):\n",
    "    sys.argv=sys.argv[:1]\n",
    "    \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-e', '--epochs', type=int, default=10000,\n",
    "    help='number of epochs for training')\n",
    "\n",
    "parser.add_argument('-l', '--learn_rate', default=0.0001, type=float,\n",
    "    help='the learning rate for the network')\n",
    "\n",
    "parser.add_argument('-n', '--num_ims', default=30, type=int,\n",
    "    help='number of total images')\n",
    "\n",
    "parser.add_argument('-f', '--num_filters', default=128, type=int,\n",
    "    help='number of filters in each conv layer')\n",
    "\n",
    "parser.add_argument( '--fc', default=512, type=int,\n",
    "    help='number of fully connected units')\n",
    "\n",
    "parser.add_argument('--coord_penalty', default=5, type=int,\n",
    "    help='penalty for guessing coordinates wrong')\n",
    "\n",
    "parser.add_argument('--size_penalty', default=5, type=int,\n",
    "    help='penalty for guessing height or width wrong')\n",
    "\n",
    "parser.add_argument('--nonobj_penalty', default=0.5, type=float,\n",
    "    help='penalty for guessing an object where one isnt')\n",
    "\n",
    "parser.add_argument('-c','--num_extra_conv', default=0, type=int,\n",
    "    help='conv layers to add on to each conv layer before max pooling')\n",
    "\n",
    "parser.add_argument('--num_convpool', default=4, type=int,\n",
    "    help='number of conv layer-pool layer pairs')\n",
    "\n",
    "parser.add_argument('--momentum', default=0.9, type=float,\n",
    "    help='momentum')\n",
    "\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u1/r/racah/projects/mantissa-climate/results/run101\n",
      "Building model and compiling functions...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10000 took 5.821s\n",
      "\ttraining los:\t\t11.9792\n",
      "\ttraining acc:\t\t0.0237 %\n",
      "  validation loss:\t\t12.390483\n",
      "  validation accuracy:\t\t0.22 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0205dcd170f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m'''train'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_ims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_ims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/global/u1/r/racah/projects/mantissa-climate/notebooks/train_val.ipynb\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(datasets, network, fns, num_epochs, num_ims, save_weights, save_plots, save_path, batchsize, load_path)\u001b[0m\n",
      "\u001b[0;32m/global/u1/r/racah/projects/mantissa-climate/notebooks/train_val.ipynb\u001b[0m in \u001b[0;36mdo_one_epoch\u001b[0;34m(epoch, num_epochs, x_train, y_train, x_val, y_val, batchsize, train_fn, val_fn, train_errs, train_accs, val_errs, val_accs, val_counter, logger, num_ims)\u001b[0m\n",
      "\u001b[0;32m/global/u1/r/racah/projects/mantissa-climate/notebooks/train_val.ipynb\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(x, y, batchsize, train_fn, val_fn, num_ims)\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/cori/software/python/2.7-anaconda/envs/deeplearning/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "run_dir = create_run_dir()\n",
    "print run_dir\n",
    "\n",
    "dataset = load_precomputed_data(paths=[\"/global/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/caffe_data/chur_train.h5\",\n",
    "                                      \"/global/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/caffe_data/chur_val.h5\" ],\n",
    "                                out_of_core=True)\n",
    "# dataset = load_data(num_ims=args.num_ims,\n",
    "#                     path='/project/projectdirs/dasrepo/gordon_bell/climate/data/detection/hur_train_val.h5')\n",
    "\n",
    "'''size of ground truth grid'''\n",
    "grid_size = dataset[1].shape[1]\n",
    "\n",
    "'''set params'''\n",
    "network_kwargs = {'learning_rate': args.learn_rate, \n",
    "                  'dropout_p': 0, \n",
    "                  'weight_decay': 0, \n",
    "                  'num_filters': args.num_filters, \n",
    "                  'num_fc_units': args.fc, \n",
    "                  'num_convpool': args.num_convpool,\n",
    "                  'num_extra_conv': args.num_extra_conv,\n",
    "                  'momentum': args.momentum,\n",
    "                  'coord_penalty': args.coord_penalty,\n",
    "                  'nonobj_penalty': args.nonobj_penalty,\n",
    "                   }\n",
    "\n",
    "\n",
    "'''get network and train_fns'''\n",
    "train_fn, val_fn, box_fn, network, hyperparams = build_network(**network_kwargs)\n",
    "\n",
    "hyperparams.update({'num_ims': args.num_ims, 'tr_size': dataset[0].shape[0]})\n",
    "'''save hyperparams'''\n",
    "dump_hyperparams(hyperparams, path=run_dir)\n",
    "\n",
    "'''train'''\n",
    "train(dataset, network=network, fns=(train_fn, val_fn, box_fn),num_ims=args.num_ims, save_weights=True, num_epochs=args.epochs, save_path=run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
