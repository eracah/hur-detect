{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from os import listdir, system\n",
    "from os.path import isfile, join, isdir\n",
    "import re\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "import imp\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gzip\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"..\")\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 20\n",
    "import pdb\n",
    "import itertools\n",
    "import datetime as dt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you view a map:\n",
    " * longitude: horizontal\n",
    " * latitude: vertical\n",
    " \n",
    "Defined Here:\n",
    " * longitude (horiz): y\n",
    " * latitude (vertical): x\n",
    " \n",
    "Array-wise:\n",
    " * dim1(x) : vertical\n",
    " * dim2(y) : horizontal\n",
    " \n",
    "So:\n",
    " * dim1 of array is latitude (thus x)\n",
    " * dim2 is longitude (thus y)\n",
    "\n",
    "So if we define something as xmin,xmax,ymin,ymax here:\n",
    " * filling in that box in the array is:\n",
    "    * arr[xmin:xmax, ymin:ymax] = 0\n",
    "    \n",
    "    \n",
    "* and the array is 768,1152 ?\n",
    "\n",
    "\n",
    "\n",
    "#LABEL NUMBERS\n",
    "* Tropical Depression is 1\n",
    "* Hurricane is 2\n",
    "* ETC is 3\n",
    "* AR is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The etc files that use even time steps for labels:\n",
    "#1979, 1980, 1982, 1983, 1984, 1985"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gbdl = \"/project/projectdirs/dasrepo/gordon_bell/deep_learning/\"\n",
    "# imdir = gbdl + \"data/climate/CAM5_0.25/climo/big_images/\"\n",
    "\n",
    "gbdl = \"/storeSSD/eracah/nersc/data/\"\n",
    "imdir = \"/storeSSD/cbeckham/nersc/big_images/1979/\"\n",
    "#ds = nc.Dataset(join(imdir,\"netcdf_files\", \"cam5_1_amip_run2.cam2.h2.1984-09-06-00000.nc\"))\n",
    "metadatadir = join(gbdl, \"teca_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_labels_for_dataset(fname, metadata_dir, time_steps=8):\n",
    "    '''takes in string for fname and the number of time_steps and outputs\n",
    "    a time_steps by maximages by 5 tensor encoding the coordinates and class of each event in a time step'''\n",
    "   \n",
    "    weather_types = ['tc','etc', 'us-ar']\n",
    "    ts=get_timestamp(fname)\n",
    "    maximagespertimestep=25\n",
    "    \n",
    "    # for every time step for every possible event, xmin,xmax,ymin,ymax,class\n",
    "    bboxes = np.zeros((time_steps, maximagespertimestep, 5))\n",
    "    event_counter = np.zeros((time_steps,))\n",
    "    for weather_type in weather_types:\n",
    "        selectdf = match_nc_to_csv(fname, metadata_dir, weather_type)\n",
    "    \n",
    "        timelist=set(selectdf[\"time_step\"])\n",
    "        for t in timelist:\n",
    "            t = int(t)\n",
    "\n",
    "            coords_for_t = selectdf[selectdf[\"time_step\"]==t].drop([\"time_step\"], axis=1).values\n",
    "            coords_for_t = coords_for_t[(coords_for_t > 0).all(1)]\n",
    "\n",
    "            # get current number of events and number of events for this time step\n",
    "            num_events_for_t = coords_for_t.shape[0]\n",
    "            cur_num_events = int(event_counter[t])\n",
    "            \n",
    "            #make slice\n",
    "            slice_for_t = slice(cur_num_events, cur_num_events + num_events_for_t)\n",
    "\n",
    "            #fill variables\n",
    "            bboxes[t, slice_for_t] = coords_for_t\n",
    "            event_counter[t] += num_events_for_t\n",
    "    return bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def match_nc_to_csv(fname,metadata_dir,weather_type, inc_csv=False):\n",
    "    coord_keys = [\"xmin\", \"xmax\", \"ymin\", \"ymax\"]\n",
    "    ts=get_timestamp(fname)\n",
    "\n",
    "    if weather_type == 'us-ar':\n",
    "        labeldf = pd.read_csv(join(metadata_dir, 'ar_labels.csv'))\n",
    "        tmplabeldf=labeldf.ix[ (labeldf.month==ts.month) & (labeldf.day==ts.day) & (labeldf.year==ts.year) ].copy()\n",
    "    else:\n",
    "        labeldf = pd.read_csv(join(metadata_dir, '_'.join([str(ts.year),weather_type, 'labels.csv'])))\n",
    "        tmplabeldf=labeldf.ix[ (labeldf.month==ts.month) & (labeldf.day==ts.day) ].copy()\n",
    "    \n",
    "    \n",
    "    selectdf=tmplabeldf[[\"time_step\"]+ coord_keys + [\"category\"]]\n",
    "    if inc_csv is True:\n",
    "        return selectdf, labeldf\n",
    "    else:\n",
    "        return selectdf \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_timestamp(filename):\n",
    "    rpyear = re.compile(r\"(\\.h2\\.)(.*?)(-)\")\n",
    "    rpdaymonth = re.compile(r\"(-)(.*?)(\\d{5}\\.)\")\n",
    "    year=int(rpyear.search(filename).groups()[1])\n",
    "    tmp=rpdaymonth.search(filename).groups()[1].split('-')\n",
    "    month=int(tmp[0])\n",
    "    day=int(tmp[1])\n",
    "    return dt.date(year,month,day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_bbox_minmax_to_cent_xywh(bboxes):\n",
    "    #current bbox set up is xmin,ymin,xmax,ymax\n",
    "    xmin, xmax,ymin,  ymax = [ bboxes[:,:,i] for i in range(4) ]\n",
    "    \n",
    "    w = xmax - xmin\n",
    "    h = ymax - ymin\n",
    "\n",
    "    x_c = xmin + w / 2.\n",
    "    y_c = ymin + h / 2.\n",
    "    \n",
    "    \n",
    "    bboxes[:,:,0] = x_c\n",
    "    bboxes[:,:,1] = y_c\n",
    "    bboxes[:,:,2] = w # w\n",
    "    bboxes[:,:,3] = h #h\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert_bbox_minmax_to_cent_xywh(make_labels_for_dataset(\"cam5_1_amip_run2.cam2.h2.1984-01-03-00000.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bbox_t = convert_bbox_minmax_to_cent_xywh(make_labels_for_dataset(\"cam5_1_amip_run2.cam2.h2.1984-01-03-00000.nc\"))\n",
    "\n",
    "# grid_t = make_yolo_masks_for_dataset(\"cam5_1_amip_run2.cam2.h2.1984-01-03-00000.nc\")\n",
    "\n",
    "# bbox =bbox_t[0,2]\n",
    "# grid = grid_t[0]\n",
    "\n",
    "# test_grid(bbox,grid,768,1152,64,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_grid(bbox, grid, xdim, ydim, scale_factor,num_classes, caffe_format=False):\n",
    "    cls = int(bbox[4])\n",
    "    x,y = bbox[0] / scale_factor, bbox[1] / scale_factor\n",
    "    xo,yo = (bbox[0] % scale_factor) / float(scale_factor), (bbox[1] % scale_factor) / float(scale_factor)\n",
    "    w,h = bbox[2] / scale_factor / (xdim / scale_factor), bbox[3] / scale_factor/ (ydim / scale_factor)\n",
    "    \n",
    "    depth = 5 + num_classes\n",
    "    if caffe_format:\n",
    "        l_box = grid[:depth,x,y]\n",
    "    else:\n",
    "        l_box = grid[int(x),int(y),:depth]\n",
    "    lbl = num_classes*[0]\n",
    "    lbl[cls-1] = 1\n",
    "    \n",
    "    real_box = [xo,yo,w,h,1.]\n",
    "    real_box.extend(lbl)\n",
    "    \n",
    "    print l_box\n",
    "    print real_box\n",
    "    assert np.allclose(l_box, real_box), \"Tests Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_detection_gr_truth(xdim, ydim, scale_factor, bbox_tensor, num_classes):\n",
    "    #x_xy : 1,2 tuple with x and y sizes for image\n",
    "    #scale_factor: factor to scale xy size by fro gr_truth grid for YOLO\n",
    "    #scale_factor = float(scale_factor)\n",
    "    # xdim, ydim = 768,1152\n",
    "    # scale_factor = 64\n",
    "    # bbox_tensor = make_labels_for_dataset(\"cam5_1_amip_run2.cam2.h2.1984-01-03-00000.nc\")\n",
    "    # num_classes = 4 \n",
    "\n",
    "    scale_factor = float(scale_factor)\n",
    "    bbox_classes = bbox_tensor[:,:,4]\n",
    "    bbox_coords = bbox_tensor[:,:,:4]\n",
    "\n",
    "    #make sure xy coords divide cleanly with scale_factor\n",
    "    assert xdim % scale_factor == 0 and ydim % scale_factor == 0, \"scale factor %i must divide the xy (%i, %i) coords cleanly \" %(scale_factor,xdim, ydim)\n",
    "\n",
    "\n",
    "    x_len,y_len = xdim / int(scale_factor), ydim / int(scale_factor)\n",
    "    last_dim = 5 + num_classes #x,y,w,h,c plus num_classes for one hot encoding\n",
    "\n",
    "\n",
    "    #divide up bbox with has range 0-95 to 0-95/scale_factor (so 6x6 for scale factor of 16)\n",
    "    bb_scaled = bbox_coords / scale_factor\n",
    "    \n",
    "\n",
    "    #each coordinate goes at index i,j in the 6x6 array, where i,j are the coordinates of the\n",
    "    #lower left corner of the grid that center of the box (in 6x6 space ) falls on\n",
    "    #subtract eps so we dont't have one off error\n",
    "    eps = np.finfo(float).eps\n",
    "    inds = np.floor(bb_scaled[:,:,:2]-10*eps).astype('int')\n",
    "\n",
    "    #xywh where x and y are offset from lower left corner of grid thay are in [0,1] and w and h\n",
    "    # are what fraction the width and height of bboxes are of the total width and total height of the image\n",
    "    xywh = np.copy(bb_scaled)\n",
    "\n",
    "    #subtract the floored values to get the offset from the grid cell\n",
    "    xywh[:,:,:2] -= inds[:,:,:2].astype('float')\n",
    "\n",
    "\n",
    "    #divide by scaled width and height to get wdith and height relative to width and height of image (width is just xrange, height is yrange)\n",
    "    xywh[:,:,2] /= x_len\n",
    "    xywh[:,:,3] /= y_len\n",
    "\n",
    "\n",
    "    #make gr_truth which is \n",
    "\n",
    "    gr_truth = np.zeros((bbox_coords.shape[0],last_dim, x_len, y_len ))\n",
    "#     else:\n",
    "#         gr_truth = np.zeros((bbox_coords.shape[0], x_len,y_len,last_dim))\n",
    "\n",
    "\n",
    "    #sickens me to a do a for loop here, but numpy ain't cooperating\n",
    "    # I tried gr_truth[np.arange(gr_truth.shape[0]),inds[:0], inds[:1]][:,4] = xywh\n",
    "    #but it did not work\n",
    "\n",
    "    # we assume one box per image here\n",
    "    # for each grid point that is center of image plop in center, and width and height and class\n",
    "    for i in range(gr_truth.shape[0]):\n",
    "        #put coordinates, conf and class for all events (now there are multiple)\n",
    "        for j, coords in enumerate(xywh[i]):\n",
    "\n",
    "\n",
    "            # the index into the groudn truth grid where class should go\n",
    "            xind, yind = inds[i,j,0], inds[i,j,1]\n",
    "            gr_truth[i, :4, xind,yind,] = coords\n",
    "\n",
    "            #put in confidence\n",
    "            gr_truth[i,4,xind,yind] = 1 if bbox_classes[i,j] > 0. else 0.\n",
    "\n",
    "            #put in class label\n",
    "            gr_truth[i, 4 + int(bbox_classes[i,j]),xind,yind] = 1. if bbox_classes[i,j] > 0. else 0.\n",
    "\n",
    "    return gr_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "          \n",
    "def make_yolo_masks_for_dataset(camfile_name,metadata_dir=\"/storeSSD/eracah/data/teca_metadata/\", xdim=768, ydim=1152,time_steps=8, classes=4):\n",
    "    \n",
    "    labels_tensor = make_labels_for_dataset(camfile_name,metadata_dir, time_steps)\n",
    "    labels_tensor = convert_bbox_minmax_to_cent_xywh(labels_tensor)\n",
    "\n",
    "\n",
    "    yolo_mask = create_detection_gr_truth(xdim,ydim,scale_factor=64.,bbox_tensor = labels_tensor, num_classes=classes)\n",
    "    \n",
    "    #masks is an 8,num_classes, 768, 1152 mask 0's everywhere except where class is\n",
    "    return yolo_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# match_nc_to_csv(\"cam5_1_amip_run2.cam2.h2.1984-09-06-00000.nc\", \"us-ar\", inc_csv=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_time_slice(dataset, time, variables, x=768, y=1152):\n",
    "    '''Takes in a dataset, a time and variables and gets one time slice for all the variables and all x and y'''\n",
    "    variables_at_time_slice = [dataset[k][time] for k in variables]\n",
    "    tensor = np.vstack(variables_at_time_slice).reshape(len(variables), x,y)\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "\n",
    "def make_spatiotemporal_tensor(dataset,num_time_slices, variables, x=768, y=1152):\n",
    "    '''takes in: dataset, num_time_slices\n",
    "       returns: num_time_slices, num_variables,x,y'''\n",
    "    time_slices = [ make_time_slice(dataset, time, variables) for time in range(num_time_slices) ]\n",
    "    tensor = np.vstack(time_slices).reshape(num_time_slices, len(variables), x, y)\n",
    "\n",
    "    return tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _day_iterator(years=[1979], metadata_dir=\"/storeSSD/eracah/data/teca_metadata/\", data_dir=\"/project/projectdirs/dasrepo/gordon_bell/climate/data/big_images/\",\n",
    "                  shuffle=False, days=365, time_steps=8, classes=2, labels_only=True):\n",
    "    \"\"\"\n",
    "    This iterator will return a pair of  tensors:\n",
    "       * one is dimension (8, 16, 768, 1152) \n",
    "       * the other is dimension (8,12,18,9) \n",
    "               -> 8 time steps, downsampled x, downsampled y, (xoffset, yoffset, w, h, confidence, softmax for 4 classes)\n",
    "    each tensor corresponding to one of the 365 days of the year\n",
    "    \"\"\"\n",
    "    variables = [u'PRECT',\n",
    "                 u'PS',\n",
    "                 u'PSL',\n",
    "                 u'QREFHT',\n",
    "                 u'T200',\n",
    "                 u'T500',\n",
    "                 u'TMQ',\n",
    "                 u'TREFHT',\n",
    "                 u'TS',\n",
    "                 u'U850',\n",
    "                 u'UBOT',\n",
    "                 u'V850',\n",
    "                 u'VBOT',\n",
    "                 u'Z1000',\n",
    "                 u'Z200',\n",
    "                 u'ZBOT']    \n",
    "    # this directory can be accessed from cori\n",
    "    maindir = data_dir #+ str(year) \n",
    "    lsdir=listdir(maindir)\n",
    "    rpfile = re.compile(r\"^cam5_.*\\.nc$\")\n",
    "    camfiles = [f for f in lsdir if rpfile.match(f)]\n",
    "    camfiles = [c for c in camfiles if get_timestamp(c).year in years]\n",
    "    \n",
    "    camfiles.sort()\n",
    "    camfiles = camfiles[:days]\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(camfiles)\n",
    "        #sys.stderr.write(\"warning: shuffling camfiles in _day_iterator()\\n\")\n",
    "    for camfile in camfiles:\n",
    "        dataset = nc.Dataset(maindir+'/'+camfile, \"r\", format=\"NETCDF4\")\n",
    "        x=768\n",
    "        y=1152\n",
    "        day_slice = make_spatiotemporal_tensor(dataset,time_steps,variables) #one day slice per dataset\n",
    "        tr_data = day_slice.reshape(time_steps,len(variables), x, y)\n",
    "        masks = make_yolo_masks_for_dataset(camfile,metadata_dir=metadata_dir, xdim=768, ydim=1152,time_steps=8, classes=classes)\n",
    "        if labels_only:\n",
    "            # we assume labels are evn time steps here\n",
    "            tr_data = tr_data[[0,2,4,6]]\n",
    "            masks = masks[[0,2,4,6]]\n",
    "        \n",
    "        yield tr_data, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_iterator(batch_size,\n",
    "                  data_dir=\"/project/projectdirs/dasrepo/gordon_bell/deep_learning/data/climate/big_images/\",\n",
    "                  metadata_dir=\"/storeSSD/eracah/data/teca_metadata/\",\n",
    "                  time_chunks_per_example=8,\n",
    "                  shuffle=False,\n",
    "                  days=365,\n",
    "                  years=[1979],\n",
    "                  time_steps=8, classes=2, labels_only=True):\n",
    "    '''\n",
    "    Args:\n",
    "       batch_size: number of examples in a batch\n",
    "       data_dir: base dir where data is\n",
    "       time_chunks_per_example: how many time steps are in a given example (default is one, but when we do 3D conv -> move to >1)\n",
    "                            - should divide evenly into 8\n",
    "    '''\n",
    "    # for each day (out of 365 days)\n",
    "    day=0\n",
    "    for tensor, masks in _day_iterator(years=years,data_dir=data_dir,metadata_dir=metadata_dir,\n",
    "                                       shuffle=shuffle,classes=classes,days=days, \n",
    "                                       time_steps=time_steps,\n",
    "                                       labels_only=labels_only):  #tensor is 8,16,768,1152\n",
    "        # preprocess for day\n",
    "        tensor, min_, max_ = normalize(tensor)\n",
    "        \n",
    "        #TODO: preprocess over everything\n",
    "        #TODO: split up into train,test, val\n",
    "        time_chunks_per_day, variables, h, w = tensor.shape #time_chunks will be 8\n",
    "        assert time_chunks_per_day % time_chunks_per_example == 0, \"For convenience, \\\n",
    "        the time chunk size should divide evenly for the number of time chunks in a single day\"\n",
    "        \n",
    "        #reshapes the tensor into multiple spatiotemporal chunks of (chunk_size, 16, 768,1152)\n",
    "        spatiotemporal_tensor = tensor.reshape(time_chunks_per_day / time_chunks_per_example, \n",
    "                                               time_chunks_per_example, variables, h ,w)\n",
    "\n",
    "        sp_mask = masks\n",
    "        sp_mask = masks.reshape(time_chunks_per_day / time_chunks_per_example, time_chunks_per_example,*masks.shape[1:])\n",
    "        \n",
    "        #if shuffle:\n",
    "        #    np.random.shuffle(spatiotemporal_tensor)\n",
    "\n",
    "        b = 0\n",
    "        while True:\n",
    "            if b*batch_size >= spatiotemporal_tensor.shape[0]:\n",
    "                break\n",
    "            # todo: add labels\n",
    "\n",
    "            yield spatiotemporal_tensor[b*batch_size:(b+1)*batch_size], sp_mask[b*batch_size:(b+1)*batch_size]\n",
    "            b += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(arr,min_=None, max_=None, axis=(0,2,3)):\n",
    "        if min_ is None or max_ is None:\n",
    "            min_ = arr.min(axis=(0,2,3), keepdims=True)\n",
    "\n",
    "            max_ = arr.max(axis=(0,2,3), keepdims=True)\n",
    "\n",
    "        midrange = (max_ + min_) / 2.\n",
    "\n",
    "        range_ = (max_ - min_) / 2.\n",
    "        \n",
    "        arr -= midrange\n",
    "\n",
    "        arr /= (range_)\n",
    "        return arr, min_, max_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_coord_tens_to_box(coord_tens, xind, yind, scale_factor, xdim=768,ydim=1152):\n",
    "    \n",
    "    \n",
    "    xoff,yoff,w,h = coord_tens\n",
    "    \n",
    "    x,y = xind+ xoff, yind+ yoff\n",
    "    \n",
    "    x,y,w,h = [scale_factor * c for c in [x,y,(xdim/scale_factor)*w,(ydim/scale_factor)*h] ]\n",
    "    \n",
    "    return x,y,w,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "\n",
    "#tropical depression are 0\n",
    "# hurricanes are 1\n",
    "\n",
    "\n",
    "def bbox_iterator(years,days,\n",
    "                  batch_size = 1,\n",
    "                  data_dir=\"/storeSSD/eracah/data/netcdf_ims/\", \n",
    "                  metadata_dir=\"/storeSSD/eracah/data/metadata/\",\n",
    "                shuffle=False, num_classes=4, labels_only=True, time_chunks_per_example=1 ):\n",
    "    \n",
    "    \"\"\"years: list of years,\n",
    "        days: number of days\n",
    "        classes: number of classes\n",
    "        labels_only: -> if true -> only does images with labels\"\"\"\n",
    "    for x,y in data_iterator(years=years,batch_size=batch_size, data_dir=data_dir, metadata_dir=metadata_dir, time_chunks_per_example=time_chunks_per_example,\n",
    "                  shuffle=shuffle,days=days, classes=num_classes, labels_only=labels_only):\n",
    "\n",
    "            x, y = np.swapaxes(x, 1, 2), y\n",
    "            y = y.astype(\"float32\")\n",
    "            yield x, y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': '/storeSSD/eracah/data/netcdf_ims/', 'labels_only': False, 'metadata_dir': '/storeSSD/eracah/data/metadata', 'time_chunks_per_example': 8, 'days': 5, 'years': [1980]}\n",
      "(1, 16, 8, 768, 1152) (1, 8, 9, 12, 18)\n",
      "(1, 16, 8, 768, 1152) (1, 8, 9, 12, 18)\n",
      "(1, 16, 8, 768, 1152) (1, 8, 9, 12, 18)\n",
      "(1, 16, 8, 768, 1152) (1, 8, 9, 12, 18)\n",
      "(1, 16, 8, 768, 1152) (1, 8, 9, 12, 18)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dir_kwargs = dict(data_dir=\"/storeSSD/eracah/data/netcdf_ims/\", metadata_dir=\"/storeSSD/eracah/data/metadata\")\n",
    "    tr_kwargs = dict(years=[1980], days=5, time_chunks_per_example=8, labels_only=False)\n",
    "    tr_kwargs.update(dir_kwargs)\n",
    "    val_kwargs= dict(years=[1979], days=2)\n",
    "    val_kwargs.update(dir_kwargs)\n",
    "    print tr_kwargs\n",
    "    for x,y in bbox_iterator(**tr_kwargs):\n",
    "        print x.shape, y.shape\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
