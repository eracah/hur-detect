{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../notebooks_src/metrics/mAP.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/postprocessing/utils.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/ssd/encode.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/ssd/make_anchors_orig.ipynb\n",
      "box_encode_decode_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/box_encode_decode_configs.ipynb\n",
      "tensorboard_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/tensorboard_configs.ipynb\n",
      "fit_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/fit_configs.ipynb\n",
      "labels_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/labels_configs.ipynb\n",
      "load_data_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/load_data_configs.ipynb\n",
      "losses_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/losses_configs.ipynb\n",
      "metrics_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/metrics_configs.ipynb\n",
      "models_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/models_configs.ipynb\n",
      "optimizers_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/configs/optimizers_configs.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/load_data/get_generator.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/load_data/generator/generator.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/util.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/load_data/generator/batch_fetcher.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/ssd/utils.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/ssd/decode.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/postprocessing/ssd.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/tf_extended/math.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/tf_extended/tensors.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/postprocessing/bboxes.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/postprocessing/unpack.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/metrics/utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from nbfinder import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "if __name__ == \"__main__\":\n",
    "    sys.path.append(\"../../\")\n",
    "#from notebooks_src.metrics.mAP import calc_batch_metrics, EpochMetrics, calc_ap_one_class\n",
    "from notebooks_src.configs import configs\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(model, generator, val_generator,num_epochs, loss_func, opt):\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        tr_steps_per_epoch= generator.num_ims / generator.batch_size\n",
    "        val_steps_per_epoch = val_generator.num_ims / val_generator.batch_size\n",
    "        \n",
    "\n",
    "        y_true, y_preds = get_y_true_y_preds_tensors(model, generator.batch_size,generator.data.labels.shape[1:])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #running_average_loss = tf.placeholder(dtype=tf.float32,shape=())\n",
    "        loss_tensor = loss_func(y_true, y_preds)\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            tf.summary.scalar(\"loss\", loss_tensor)\n",
    "            #tf.summary.scalar(\"running_average_loss\", running_average_loss)\n",
    "            \n",
    "            \n",
    "        \n",
    "#         with tf.name_scope(\"accuracy\"):\n",
    "#             accuracy_tensor = tf.placeholder(dtype=tf.float32, shape=())\n",
    "#             tf.summary.scalar(\"accuracy\", accuracy_tensor)\n",
    "    \n",
    "        summaries_dir = get_summaries_dir()\n",
    "        train_writer = tf.summary.FileWriter(summaries_dir + '/train',\n",
    "                                      sess.graph)\n",
    "        train_epoch_writer = tf.summary.FileWriter(summaries_dir + '/train_epoch',\n",
    "                                      sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(summaries_dir + '/val')\n",
    "        val_epoch_writer = tf.summary.FileWriter(summaries_dir + '/val_epoch',\n",
    "                                      sess.graph)\n",
    "        \n",
    "        \n",
    "        input_ = model.input\n",
    "\n",
    "        train_step = opt.minimize(loss_tensor)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        tr_global_step_counter = 0\n",
    "        val_global_step_counter = 0\n",
    "        print \"beginning training\"\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            t0 = time.time()\n",
    "            \n",
    "            \n",
    "            tr_global_step_counter =  run_loss_loop(type_=\"tr\", epoch=epoch, steps_per_epoch=tr_steps_per_epoch, \n",
    "                                                    step_writer=train_writer, \n",
    "                                                    epoch_writer=train_epoch_writer, generator=generator, \n",
    "                                                    train_step=train_step, loss_tensor=loss_tensor, summary_op=merged, \n",
    "                                                    input_=input_, y_true=y_true, global_step=tr_global_step_counter, sess=sess)\n",
    "            \n",
    "            \n",
    "            \n",
    "            t1 = time.time()\n",
    "            epoch_time = t1-t0\n",
    "            print \"epoch time: \", epoch_time\n",
    "            write_summary(epoch_time, \"epoch_time\", train_epoch_writer, epoch)\n",
    "            val_global_step_counter = run_loss_loop(type_=\"val\", epoch=epoch, steps_per_epoch=val_steps_per_epoch, \n",
    "                                                    step_writer=val_writer, epoch_writer=val_epoch_writer, generator=val_generator, \n",
    "                                                    train_step=None, loss_tensor=loss_tensor, summary_op=merged, \n",
    "                                                    input_=input_, y_true=y_true, global_step=val_global_step_counter,sess=sess)\n",
    "            \n",
    "#             if epoch % 10 == 0:\n",
    "#                 get_epoch_accuracy(generator, model, sess, input_,train_epoch_writer, epoch)\n",
    "#                 get_epoch_accuracy(val_generator, model, sess, input_, val_epoch_writer, epoch)\n",
    "            \n",
    "  \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_summary(value, name, writer, index):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        writer.add_summary(summary, index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_loss_loop(type_, epoch, steps_per_epoch, step_writer, epoch_writer, generator, \n",
    "                  train_step, loss_tensor, summary_op, input_, y_true, global_step, sess):\n",
    "    \n",
    "    if type_ == \"tr\":\n",
    "        sess_list = [train_step,loss_tensor, summary_op]\n",
    "    else:\n",
    "        sess_list = [loss_tensor,loss_tensor, summary_op]\n",
    "    \n",
    "    loss_sum = 0.0\n",
    "    for step in range(steps_per_epoch):\n",
    "        im, boxes = generator.next()\n",
    "\n",
    "        _, cur_loss, summary = sess.run(sess_list,feed_dict={input_:im, \n",
    "                                                            y_true:boxes})\n",
    "        print cur_loss\n",
    "        loss_sum += cur_loss\n",
    "\n",
    "\n",
    "        step_writer.add_summary(summary,global_step)\n",
    "        global_step += 1\n",
    "    average_loss = loss_sum / float(steps_per_epoch)\n",
    "    print \"at epoch %i, the loss for %s is %8.4f\" %(epoch,type_, average_loss)\n",
    "    write_summary(average_loss,\"running_average_loss\", epoch_writer, epoch)\n",
    "    return global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../../notebooks_src/metrics/mAP.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/postprocessing/utils.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/ssd/encode.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/ssd/make_anchors_orig.ipynb\n",
      "box_encode_decode_configs\n",
      "box_encode_decode_configs\n",
      "tensorboard_configs\n",
      "fit_configs\n",
      "labels_configs\n",
      "load_data_configs\n",
      "losses_configs\n",
      "metrics_configs\n",
      "models_configs\n",
      "optimizers_configs\n",
      "tensorboard_configs\n",
      "fit_configs\n",
      "labels_configs\n",
      "load_data_configs\n",
      "losses_configs\n",
      "metrics_configs\n",
      "models_configs\n",
      "optimizers_configs\n",
      "importing Jupyter notebook from ../../notebooks_src/load_data/get_generator.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/load_data/generator/generator.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/util.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/load_data/generator/batch_fetcher.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/ssd/utils.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/box_encode_decode/ssd/decode.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/postprocessing/ssd.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/tf_extended/math.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/tf_extended/tensors.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/postprocessing/bboxes.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/postprocessing/unpack.ipynb\n",
      "importing Jupyter notebook from ../../notebooks_src/metrics/utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "            \n",
    "                \n",
    "\n",
    "def get_summaries_dir():\n",
    "    if configs[\"exp_name\"] == \"None\":\n",
    "        exp_name = \"_\".join([configs[\"base_model\"], configs[\"detection_model\"]]) + \"_\" + str(int(time.time()))\n",
    "    else:\n",
    "        exp_name = configs[\"exp_name\"]\n",
    "    return join(configs[\"logs_dir\"],exp_name )\n",
    "\n",
    "def get_epoch_accuracy(generator, model, sess,input_,writer,epoch):\n",
    "    epm = EpochMetrics()\n",
    "    batch_accuracy_tensors, y_true = get_batch_accuracy_tensors(calc_batch_metrics, model, generator)\n",
    "    steps_per_epoch = generator.num_ims / generator.batch_size\n",
    "    for step in range(steps_per_epoch):\n",
    "        im, boxes = generator.next()\n",
    "        updated_metrics = sess.run(batch_accuracy_tensors, feed_dict={y_true:boxes, input_:im})\n",
    "        epm.update_metrics(*updated_metrics)\n",
    "\n",
    "    final_metrics = epm.get_final_metrics()\n",
    "    aps_voc12, placeholders= calc_ap_one_class()\n",
    "    all_aps12 = {}\n",
    "\n",
    "\n",
    "    for c in final_metrics[0].keys():\n",
    "        placefillers = [d[c] for d in final_metrics]\n",
    "        all_aps12[c] = sess.run(aps_voc12, feed_dict = dict(zip(placeholders, placefillers)) )\n",
    "\n",
    "\n",
    "    mAP = np.mean(all_aps12.values())\n",
    "    write_summary(mAP,\"accuracy\", writer, epoch)\n",
    "    for cls, ap in all_aps12.iteritems():\n",
    "        write_summary(ap,\"class \" + str(cls) + \" accuracy\", writer, epoch)\n",
    "\n",
    "def get_y_true_y_preds_tensors(model, batch_size, label_shape):\n",
    "    output_tensors = model.outputs\n",
    "        \n",
    "    label_batch_shape = tuple([batch_size] + list(label_shape))\n",
    "        \n",
    "        \n",
    "    label_tensor = tf.placeholder(tf.float32,shape=label_batch_shape, name=\"label\")\n",
    "    return label_tensor, output_tensors\n",
    "\n",
    "def get_batch_accuracy_tensors(acc_func, model,generator):\n",
    "        batch_size = generator.batch_size\n",
    "        y_true, y_preds = get_y_true_y_preds_tensors(model, batch_size,generator.data.labels.shape[1:])\n",
    "        batch_metrics = calc_batch_metrics(y_true, y_preds)\n",
    "        return batch_metrics, y_true\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a =tf.Variable(dtype=tf.float32, initial_value=1.0)\n",
    "\n",
    "# b = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "# eq_zero = tf.cast(tf.equal(b,0), dtype=tf.float32)\n",
    "# d=a.assign_add(1.0)\n",
    "# c=a.assign_add(-d*eq_zero)\n",
    "\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for i in range(5):\n",
    "#         for j in range(10):\n",
    "#             print sess.run(c,feed_dict={b:j})\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
